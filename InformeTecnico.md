
# Escuela Polit√©cnica Nacional

## M√©todos Num√©ricos

## Informe T√©cnico: Proyecto Blackbox S

---
**Integrantes:** Alangas√≠ Anthony, Nicole Achote, Danny Caiza

**Curso:** GR1CC

**Grupo:** 8

---

## 1. Introducci√≥n

Las redes neuronales han adquirido un papel muy importante en la modelaci√≥n de fen√≥menos complejos debido a su capacidad para representar relaciones no lineales entre variables. En este tipo de sistemas, se dispone √∫nicamente de sus entradas y salidas, pero no de una expresi√≥n anal√≠tica exacta que determine su comportamiento interno entre sus entradas y salidas. En estos casos, surge la necesidad de emplear m√©todos num√©ricos que permitan inferir o aproximar la relaci√≥n anal√≠tica que existe entre ellas.

El presente proyecto tiene como objetivo estudiar la red neuronal Blackbox S, la cual implementa una funci√≥n param√©trica $$fŒ∏:R2‚ÜíB$$ que asigna un valor binario a cada par de valores $$(x_1, x_2)$$con la condici√≥n $$x_1 \ge 0$$ Si bien el modelo permite obtener predicciones para cualquier punto del dominio, la dependencia anal√≠tica entre las variables $x_1$ y $x_2$ sigue sin ser conocida expl√≠citamente. Por ello, el objetivo principal es determinar la relaci√≥n matem√°tica que existe entre estas ambas variables utilizando t√©cnicas de optimizaci√≥n no lineal, fundamentadas en m√©todos num√©ricos.

Para ello se aplican dos m√©todos num√©ricos utilizados com√∫nmente en problemas de optimizaci√≥n y ajuste de modelos que son Gauss-Newton y Levenberg‚ÄìMarquardt. Estos m√©todos permiten estimar par√°metros mediante la minimizaci√≥n del error entre valores observados y valores generados por la funci√≥n objetivo. La comparaci√≥n de estos valores permite evaluar diferencias en precisi√≥n, velocidad de convergencia y estabilidad num√©rica.

Finalmente, se hace la comparaci√≥n y la interpretaci√≥n de los resultados obtenidos por cada m√©todo, determinando cual proporciona una aproximaci√≥n m√°s adecuada a la relaci√≥n anal√≠tica buscada.  Se destaca tambi√©n la importancia de los m√©todos num√©ricos para el estudio de modelos cuyo comportamiento no se presenta en forma expl√≠cita.

---

## 2. Metodolog√≠a

**2.1. Desarrollo Matem√°tico y Modelo Anal√≠tico**
**Identificaci√≥n de la funci√≥n subyacente (Sinc Amortiguada) basada en la visualizaci√≥n.**

<br>

A partir del an√°lisis gr√°fico de los puntos donde la red neuronal cambia de salida entre las clases 0 y 1, se observ√≥ que la frontera de decisi√≥n presenta un comportamiento oscilatorio y decreciente, una caracter√≠stica muy notable de funciones tipo Sinc:
$$\text{sinc}(x) = \frac{\sin(kx)}{kx}$$
En particular, la forma de las fronteras sugiere que el patr√≥n subyacente sigue un comportamiento similar a:
$$\text{sinc}(x) = \frac{\sin(10x)}{10x}$$

Sin embargo, al comparar esta funci√≥n ideal con los datos generados por la red, fue necesario introducir dos modificaciones para ajustarla correctamente:

<br>

Amortiguaci√≥n artificial (Blackbox) para evitar la singularidad en x=0:

<br>


$$\frac{1}{x} \longrightarrow \frac{1}{x + 0.1}$$

<br>

Ajuste param√©trico general para modelar correctamente la amplitud, frecuencia, desfase y desplzamiento vertical propios de la frontera aprendida:

<br>

$$X_2 = A \cdot \frac{\sin(B x_1 + C)}{x_1 + 0.1} + D$$

<br>

Este modelo constituye una **Sinc amortiguada param√©trica**, que analiza el comportamiento oscilatorio de la frontera, pero a su vez permite adaptarlo a los valores reales detectados por los m√©todos del algoritmo.

<br>

**Formulaci√≥n de las dos ecuaciones de la frontera superior e inferior**

<br>

Durante el muestreo sistem√°tico del plano $(x_1,x_2)$, la red neuronal tenia como clasificaci√≥n; **0** o **1**. A partir de esta clasificaci√≥n se identificaron dos tipos de transiciones:

<br>

**Frontera Superior ($1\longrightarrow0$)**
Corresponde a los puntos en donde, al aumentar x<sub>2</sub>, la red cambia su predicci√≥n desde 1 hacia 0.Es decir, se delimita el limite superior de la banda donde la red considera salida = 1.
Esto, en un dialecto matem√°ticoe,quiere decir que la fronte fue modelada mediante un Sinc amortiguada con par√°metros ajustados usando **curve_fit o Metodo de Levenberg-Marquardt:**

<br>

$$x_2^{up}(x_1) = A_{sup} \cdot \frac{\sin(B_{sup} \, x_1 + C_{sup})}{x_1 + 0.1} + D_{sup}$$

<br>

Los parametros A<sub>sup</sub>,B<sub>sup</sub>,C<sub>sup</sub>,D<sub>sup</sub> representan el ajuste optimo obetenido a partil del conjunto **frontera_superior**<br>

<br>

**Frontera Inferior ($0\longrightarrow1$)**

<br>

Corresponde a los punto donde, al disminuir x<sub>2</sub>, la red cambia su prediccion desde 0 hacia 1.
Define el **limite inferior** de la region donde la red activa la salida = 1.
Para esta formulacion analitica, se siguio el mismo modelo amortiguado, pero con parametros diferentes:

<br>

$$x_2^{up}(x_1) = A_{inf} \cdot \frac{\sin(B_{inf}\, x_1 + C_{inf})}{x_1 + 0.1} + D_{inf}$$

<br>

Los parametros a evaluar se obtuvieron el m√©todo de Guss-Newton y se contrastaron num√©ricamente con la aproximaci√≥n de Levenberg-Marquardt para validad la equivalencia del ajuste.

<br>

**2.2. Descripci√≥n de la Implementaci√≥n:**

<br>

**2.2.1. Muestreo de la Frontera (Doble Bisecci√≥n)**

<br>

Con la finalizaci√≥n del alcance del objetivo de obtener una representaci√≥n precisa de las fronteras de decisi√≥n de la red Neuronal BlackBox S, se implemento un algoritmo de muestreo mediante una doble bisecci√≥n. Dado que este m√©todo permite localizar con alta exactitud los puntos donde la red cambia su salida entre 0 y , lo cual defina una banda en la que la funcion de la red es igual a 1.

Dado los pasos a seguir del algoritmo, fueron:

<br>

**a) Exploraci√≥n inicial**

<br>

Para cada valor de ùë• 1 x 1 ‚Äã dentro del intervalo estudiado, se realiz√≥ un muestreo preliminar sobre un rango definido de valores de $ùë•_2$. Este muestreo permite identificar de manera aproximada la regi√≥n donde ocurre una transici√≥n abrupta en la salida de la red, ya sea:

  ‚Ä¢ **De 1 a 0**(frontera superior)

  ‚Ä¢ **De 0 a 1**(frontera inferior)

Este punto inicial sirve como referencia para el refinamiento posterior

<br>

**b) Bisecci√≥n para la Frontera Superior ($1\longrightarrow0$)** 

<br>

Una vez detectado un punto aproximado donde la red deja de clasificar como 1, se define un intervalo [x<sub>2low</sub>, ùë•<sub>2high</sub>] que contiene la transici√≥n. Sobre este intervalo se aplica el m√©todo de bisecci√≥n cl√°sica, evaluando la red en el punto medio:

$$
X_{2mid} = \frac{X_2 + X_{2high}}{2}
$$

Dependiendo del valor de la red neuronal:

  ‚Ä¢ Si ùëì(ùë•<sub>1</sub>,ùë•<sub>2mid</sub>) = 1, se actualiza el l√≠mite inferior.

  ‚Ä¢ Si ùëì(ùë•<sub>1</sub>,ùë•<sub>2mid</sub>) = 0, se actualiza el l√≠mite superior.

Este proceso se repite hasta que:

$$
|x_{2,high} - x_{2,low}| < \epsilon
$$

donde la tolerancia utilizada fue:

$$
\epsilon = 10^{-5}
$$

El valor final se registra como punto preciso de la frontera superior.

<br>

**c) Bisecci√≥n para la Frontera Inferior (0 ‚Üí 1)**

<br>

De forma an√°loga, se construy√≥ un intervalo que contiene la transici√≥n desde salida 0 hacia acceso 1. Se aplica nuevamente el m√©todo de bisecci√≥n, pero con la l√≥gica invertida:

  ‚Ä¢ Si ùëì(ùë•<sub>1</sub>,ùë•<sub>2mid</sub>) = 0, la transici√≥n esta hacia valores superiores.

  ‚Ä¢ Si ùëì(ùë•<sub>1</sub>,ùë•<sub>2mid</sub>) = 1, la transici√≥n esta hacia valores superiores.

Este proceso determina con precisi√≥n el punto que pertenece a la frontera inferior.

<br>

**2.2.2. M√©todo Num√©rico 1: Levenberg-Marquardt (L-M):** 

<br>

Con el prop√≥sito de obtener un modelo anal√≠tico que describiera con precisi√≥n la forma de la frontera inferior identificada en el proceso de muestreo, se aplic√≥ un procedimiento de regresi√≥n no lineal mediante el algoritmo de **Levenberg‚ÄìMarquardt**, implementado a trav√©s de la funci√≥n "curve_fit" del paquete **scipy.optimize**.

<br>

**a) Selecci√≥n del modelo anal√≠tico**

<br>

A partir de la visualizaci√≥n de los datos muestreados, se identific√≥ que el comportamiento de la frontera inferior sigue la estructura de una Sinc amortiguada. Para evitar la singularidad en  x<sub>1</sub1>= 0 se utiliz√≥ la siguiente formulaci√≥n:

$$
x_2 = A \, \frac{\sin(Bx_1 + C)}{x_1 + 0.1} + D
$$

donde 

A, B, C y D representan los par√°metros a estimar mediante el ajuste

<br>

**b) Formulaci√≥n del problema de minimizaci√≥n**

<br>

El objetivo del m√©todo consiste en encontrar los par√°metros que minimicen la suma de los errores cuadr√°ticos entre los datos reales  (x<sub>1,i</sub>,x<sub>2,i</sub>)obtenidos por bisecci√≥n y los valores predichos por el modelo anal√≠tico:

$$
\min_{\beta} S(\beta)
    = \sum_{i=1}^{n} \left[ x_{2,i} - f(x_{1,i};\beta) \right]^2
$$

con:

$$
\beta = (A, B, C, D)
$$

y

$$f(x_1;\beta)=A \, \frac{\sin(Bx_1 + C)}{x_1 + 0.1} + D$$

<br>

**c) Implementaci√≥n del algoritmo Levenberg‚ÄìMarquardt**

<br>

"curve_fit" implementa internamente una combinaci√≥n entre los m√©todos de **Gauss‚ÄìNewton** y **descenso del gradiente**, controlada por un par√°metro de amortiguamiento. Este enfoque h√≠brido permite:

‚Ä¢ estabilidad num√©rica en regiones no lineales del espacio de par√°metros,

‚Ä¢ convergencia r√°pida cuando la funci√≥n se aproxima a un comportamiento cuadr√°tico.

El ajuste se realiz√≥ suministrando:

‚Ä¢ los datos experimentales $(x_1, x_2)$,

‚Ä¢ la funci√≥n modelo seleccionada,

‚Ä¢ un vector inicial de par√°metros razonable.

<br>

**d) Resultados del ajuste**

<br>

El m√©todo devolvi√≥ el conjunto de par√°metros √≥ptimos:

$$
(A_{\text{inf}},\, B_{\text{inf}},\, C_{\text{inf}},\, D_{\text{inf}})
$$

los cuales constituyen la representaci√≥n cerrada de la frontera inferior de la regi√≥n donde la red neuronal predice clase 1.

Finalmente, la calidad del ajuste fue evaluada mediante el c√°lculo del **Error Cuadr√°tico Medio (MSE)**, evidenciando que el modelo Sinc amortiguado ofrece una aproximaci√≥n precisa a los datos generados por la red.

<br>

**2.2.3. M√©todo Num√©rico 2: Gauss-Newton (GN):** 

<br>

Este metodo se lo utilizo como segundo procedimiento num√©rico para ajustar los par√°metros del modelo anal√≠tico propuesto para la frontera de la funci√≥n tipo sinc amortiguada.  
El ajuste se aplic√≥ sobre los puntos muestreados de la frontera superior $(x_1, x_2)$, previamente obtenidos mediante el algoritmo de doble bisecci√≥n.

El m√©todo permiti√≥ estimar los par√°metros (A, B, C, D) del modelo:

$$ 
x_2 = A \cdot \frac{\sin(Bx_1 + C)}{x_1 + 0.1} + D
$$

al minimizar la suma de cuadrados del error entre los valores muestreados y la estructura funcional del modelo.

El m√©todo de Gauss‚ÄìNewton es un algoritmo iterativo cl√°sico para resolver problemas de regresi√≥n no lineal, en los cuales se desea estimar un conjunto de par√°metros:

$$\theta = (A, B, C, D) $$

que minimicen la funci√≥n de error de m√≠nimos cuadrados:

$$
S(\theta)=\sum_{i=1}^{n} \left[f_\theta(x_i) - y_i\right]^2
$$

donde:

‚Ä¢ $(x_i$): puntos muestreados de la frontera,

‚Ä¢ $(y_i$): valores observados (provenientes del muestreo de alta precisi√≥n),

‚Ä¢ $(f_\theta(x_i))$: modelo anal√≠tico propuesto.

<br>

**a) Linealizaci√≥n del modelo**

<br>

Gauss‚ÄìNewton se basa en aproximar la funci√≥n no lineal mediante una expansi√≥n de primer orden de Taylor alrededor de una estimaci√≥n $(\theta_k$):

$$
f_\theta(x_i) \approx f_{\theta_k}(x_i) + J_i (\theta - \theta_k)
$$

donde $(J_i)$ es la fila del Jacobiano:

$$
J_i = 
\left[
\frac{\partial f}{\partial A},
\frac{\partial f}{\partial B},
\frac{\partial f}{\partial C},
\frac{\partial f}{\partial D}
\right]_{\theta=\theta_k}
$$

Para el modelo:

$$
f_\theta(x_1)=A\cdot \frac{\sin(Bx_1 + C)}{x_1 + 0.1} + D
$$

las derivadas parciales son:

$$
\frac{\partial f}{\partial A} = \frac{\sin(Bx_1 + C)}{x_1 + 0.1}
$$

$$
\frac{\partial f}{\partial B}
= A \cdot \frac{\cos(Bx_1 + C)\, x_1}{x_1 + 0.1}
$$

$$
\frac{\partial f}{\partial C}
= A \cdot \frac{\cos(Bx_1 + C)}{x_1 + 0.1}
$$

$$
\frac{\partial f}{\partial D} = 1
$$


Estas derivadas conforman el Jacobiano evaluado en cada dato.

<br>

**b. Obtenci√≥n de la correcci√≥n de par√°metros**

<br>

Gauss‚ÄìNewton resuelve, en cada iteraci√≥n, el sistema:

$$
J^\top J \, \Delta\theta = - J^\top r
$$

donde:

‚Ä¢ $(J)$: Jacobiano evaluado en todos los puntos,

‚Ä¢ $(r)$: vector de residuos:


$$
r_i = f_{\theta_k}(x_i) - y_i
$$

La actualizaci√≥n es:

$$
\theta_{k+1} = \theta_k + \Delta\theta
$$

El proceso se repite hasta cumplir criterios de convergencia:  
‚Ä¢ peque√±a variaci√≥n en $(\theta)$ o en la funci√≥n objetivo.


El algoritmo oper√≥ sobre:

‚Ä¢ los puntos de frontera muestreados,

‚Ä¢ el modelo anal√≠tico,  

‚Ä¢ un vector inicial razonable,  

‚Ä¢ tolerancias est√°ndar.

El resultado fue un conjunto de par√°metros ( ùê¥ , ùêµ , ùê∂ , ùê∑ ) (A,B,C,D) que proporcionan una aproximaci√≥n de alta calidad a la frontera de la funci√≥n tipo sinc amortiguada, cuyos valores fueron posteriormente comparados con el ajuste obtenido mediante Levenberg‚ÄìMarquardt.

<br>

**2.3. Diagrama de Flujo / Pseudoc√≥digo.**

<br>

### PSEUDOC√ìDIGO: M√âTODO DE LEVENBERG MARQUARDT
```
ALGORITMO LevenbergMarquardt
ENTRADA:
    f(x, Œ∏): modelo no lineal
    datos (x_i, y_i)
    Œ∏‚ÇÄ: estimaci√≥n inicial de par√°metros
    Œª‚ÇÄ: par√°metro de amortiguamiento inicial
    Œµ: tolerancia de convergencia
    maxIter: n√∫mero m√°ximo de iteraciones

SALIDA:
    Œ∏*: par√°metros ajustados

INICIAR:
    Œ∏ ‚Üê Œ∏‚ÇÄ
    Œª ‚Üê Œª‚ÇÄ

REPETIR (k = 1 hasta maxIter):

    1. Calcular residuos:
           r_i = f(x_i, Œ∏) ‚àí y_i

    2. Calcular Jacobiano J evaluado en Œ∏.

    3. Construir la matriz normal modificada:
           H = J·µÄ J + Œª * I

    4. Calcular el vector de gradiente:
           g = J·µÄ r

    5. Resolver para la actualizaci√≥n:
           ŒîŒ∏ = ‚àí H‚Åª¬π g

    6. Evaluar la nueva estimaci√≥n:
           Œ∏_nueva = Œ∏ + ŒîŒ∏

    7. Calcular el nuevo error S_nuevo y compararlo con S_actual.

    8. SI S_nuevo < S_actual ENTONCES
            Œ∏ ‚Üê Œ∏_nueva
            Œª ‚Üê Œª / 10         # Disminuir amortiguamiento ‚Üí GN m√°s puro
       SINO
            Œª ‚Üê Œª * 10         # Aumentar amortiguamiento ‚Üí m√°s estable
       FIN SI

    9. Comprobar convergencia:
            SI ||ŒîŒ∏|| < Œµ ENTONCES
                TERMINAR BUCLE

HASTA cumplir tolerancia o alcanzar maxIter

RETORNAR Œ∏
FIN ALGORITMO

```

### PSEUDOC√ìDIGO: M√âTODO DE GAUSS-NEWTON
```
ALGORITMO GaussNewton
ENTRADA:
    f(x, Œ≤): modelo no lineal
    datos (x_i, y_i)
    Œ≤‚ÇÄ: estimaci√≥n inicial de par√°metros
    Œµ: tolerancia de convergencia
    maxIter: n√∫mero m√°ximo de iteraciones

SALIDA:
    Œ≤*: par√°metros ajustados

INICIAR:
    Œ≤ ‚Üê Œ≤‚ÇÄ

REPETIR (k = 1 hasta maxIter):

    1. Calcular el vector de residuos:
           r_i = y_i ‚àí f(x_i, Œ≤)

    2. Calcular el Jacobiano J evaluado en Œ≤:
           J[i, j] = ‚àÇf(x_i, Œ≤) / ‚àÇŒ≤_j

    3. Construir la matriz normal:
           H = J·µÄ ¬∑ J

    4. Construir el vector de gradiente:
           g = J·µÄ ¬∑ r

    5. Resolver para la actualizaci√≥n:
           ŒîŒ≤ = (H)‚Åª¬π ¬∑ g

    6. Actualizar par√°metros:
           Œ≤_nuevo = Œ≤ + ŒîŒ≤

    7. Criterio de convergencia:
           SI ||ŒîŒ≤|| < Œµ ENTONCES
                TERMINAR BUCLE

    8. Actualizar:
           Œ≤ ‚Üê Œ≤_nuevo

HASTA cumplir tolerancia o alcanzar maxIter

RETORNAR Œ≤
FIN ALGORITMO

```

### PSEUDOC√ìDIGO: ALGORITMO DE DOBLE BISECCI√ìN
```
ALGORITMO DobleBiseccion
ENTRADA:
    f(x1, x2): funci√≥n de salida de la red neuronal (0 √≥ 1)
    RangoX1 = [x1_min, x1_max]
    RangoX2 = [x2_min, x2_max]
    N: n√∫mero de puntos de muestreo para x1
    M: n√∫mero de pasos de muestreo grueso en x2
    Iter: n√∫mero de iteraciones de bisecci√≥n

SALIDA:
    FronteraSuperior, FronteraInferior

INICIAR:
    FronteraSuperior ‚Üê ‚àÖ
    FronteraInferior ‚Üê ‚àÖ

PARA cada valor x1 en una malla uniforme de N puntos EN RangoX1 HACER:

    1. Muestreo grueso en x2
       PARA i desde 1 hasta M-1 HACER:
            x2_a ‚Üê x2_min + (i/M)     * (x2_max - x2_min)
            x2_b ‚Üê x2_min + ((i+1)/M) * (x2_max - x2_min)

            y_a ‚Üê f(x1, x2_a)
            y_b ‚Üê f(x1, x2_b)

            SI y_a ‚â† y_b ENTONCES
                # Se encontr√≥ un intervalo con cambio de clase
                intervalo_a ‚Üê x2_a
                intervalo_b ‚Üê x2_b
                clase_inicial ‚Üê y_a

                # 2. Bisecci√≥n fina en el intervalo
                PARA k desde 1 hasta Iter HACER:
                    medio ‚Üê (intervalo_a + intervalo_b) / 2
                    y_m ‚Üê f(x1, medio)

                    SI y_m = clase_inicial ENTONCES
                        intervalo_a ‚Üê medio
                    SINO
                        intervalo_b ‚Üê medio
                    FIN SI
                FIN PARA

                x2_borde ‚Üê (intervalo_a + intervalo_b) / 2

                # 3. Clasificaci√≥n del borde
                SI y_a = 1 Y y_b = 0 ENTONCES
                    agregar (x1, x2_borde) a FronteraSuperior
                SINO SI y_a = 0 Y y_b = 1 ENTONCES
                    agregar (x1, x2_borde) a FronteraInferior
                FIN SI

            FIN SI
        FIN PARA

FIN PARA

RETORNAR FronteraSuperior, FronteraInferior
FIN ALGORITMO

```
<br>

**2.4. An√°lisis de Estabilidad y Convergencia**
<br>

**An√°lisis del M√©todo de Gauss‚ÄìNewton**
El m√©todo de Gauss‚ÄìNewton es una estrategia iterativa utilizada para resolver problemas de minimizaci√≥n no lineal de m√≠nimos cuadrados. Su convergencia se basa en la aproximaci√≥n local del modelo mediante una expansi√≥n lineal, donde la matriz Hessiana es aproximada por el producto:
<br>

$$J^{\top} J$$

<br>

Esta simplificaci√≥n permite reducir el costo computacional, pero tambi√©n introduce limitaciones respecto a la estabilidad del m√©todo. En particular, su desempe√±o es altamente dependiente de la cercan√≠a entre la estimaci√≥n inicial y el m√≠nimo verdadero. Cuando el vector inicial se encuentra dentro de una regi√≥n donde la funci√≥n objetivo es suficientemente suave y la linealizaci√≥n es v√°lida, el m√©todo exhibe **convergencia cuasi‚Äìcuadr√°tica**, lo que lo hace eficiente para problemas bien condicionados.
<br>

Sin embargo, la estabilidad del m√©todo se ve comprometida cuando la matriz:

<br>

$$
J^{\top} J
$$

<br>

es mal condicionada o cercana a la singularidad. En tales casos, los incrementos pueden crecer sin control, deteriorando la convergencia e incluso produciendo divergencias. Esta falta de robustez limita el uso pr√°ctico del m√©todo en funciones con curvatura compleja, presencia de m√∫ltiples m√≠nimos localess o residuales grandes.

<br>

El m√©todo tambi√©n es sensible al ruido en los datos, pues peque√±as perturbaciones afectan la estructura del jacobiano y, por ende, la calidad de la aproximaci√≥n del Hessiano. Por estas razones, el m√©todo de Gauss‚ÄìNewton es considerado eficiente pero d√©bilmente estable, adecuado √∫nicamente para escenarios donde el problema est√° bien condicionado y las aproximaciones lineales son v√°lidas en la regi√≥n de b√∫squeda.

<br>

**An√°lisis del M√©todo de Levenberg‚ÄìMarquardt**
<br>

El m√©todo de Levenberg‚ÄìMarquardt, tambi√©n conocido como *damped least squares*, surge como una combinaci√≥n entre el m√©todo de Gauss‚ÄìNewton y el descenso del gradiente, incorporando un par√°metro de amortiguamiento que regula la estabilidad de la actualizaci√≥n iterativa. Este par√°metro introduce un t√©rmino adicional en el sistema lineal, convirtiendo la matriz:

<br>

$$
J^{\top} J + \lambda I
$$

<br>

en una matriz siempre invertible para:

<br>

$$
\lambda > 0
$$

<br>

Gracias a esta modificaci√≥n, el m√©todo presenta una estabilidad significativamente superior en comparaci√≥n con Gauss‚ÄìNewton, incluso en situaciones donde:
<br>

$$
J^{\top} J
$$

<br>

es singular o mal condicionada. En esencia, el par√°metro de amortiguamiento act√∫a como un regulador din√°mico que controla el tama√±o del paso y evita movimientos bruscos que podr√≠an conducir a divergencias.
En t√©rminos de convergencia, el m√©todo de Levenberg‚ÄìMarquardt exhibe un comportamiento h√≠brido:

<br>

- Cuando $\lambda$ es peque√±o, el m√©todo se aproxima al comportamiento cuasi‚Äìcuadr√°tico del m√©todo de Gauss‚ÄìNewton, garantizando rapidez en la convergencia.

<br>

- Cuando la iteraci√≥n se encuentra lejos del m√≠nimo o la superficie de error presenta curvatura irregular, $\lambda$ aumenta y el m√©todo adopta un comportamiento m√°s estable, similar al descenso por gradiente.

<br>

Esto proporciona una convergencia lineal pero segura.

<br>

Esta transici√≥n autom√°tica entre rapidez y estabilidad convierte al m√©todo en un algoritmo robusto para una amplia variedad de problemas no lineales, incluso aquellos con ruido, discontinuidades suaves o condiciones iniciales poco precisas.

<br>

En resumen, Levenberg‚ÄìMarquardt es un m√©todo que combina **alta estabilidad global** con una **convergencia eficiente** en zonas localmente bien comportadas.

---
## 3. Resultados

**3.1. Ejecuci√≥n y Descripci√≥n de Casos de Prueba.**
Se ha realizado un muestreo de varios puntos utilizando el modelo para identificar la regi√≥n donde $f(x_1,x_2) = 1$. En la Figura 1, se observa que la regi√≥n tiene una forma senoidal hasta $x_1 \approx 0.9$ y luego, se mantiene de forma constante.

<br>

![Muestreo de datos](image.png)

*Figura 1 Gr√°fica del muestreo de datos resultante*

<br>

Debido a que el conjunto de puntos est√° contenido en un √°rea limitada, se aplic√≥ el m√©todo de bisecci√≥n para encontrar los puntos ubicados en la frontera de decisi√≥n donde el modelo cambia de 0 a 1 con una tolerancia de $10^{-5}$. Esto permiti√≥ obtener dos conjuntos de puntos que representan las fronteras superior e inferior del conjunto donde el modelo predice 1. Estos puntos se muestran en la Figura 2.

<br>

![Fronteras de decisi√≥n](image-1.png)

*Figura 2 Gr√°fica de las fronteras de decisi√≥n obtenidas a trav√©s del m√©todo de bisecci√≥n*

<br>

**3.2. Comparaci√≥n con Soluciones Anal√≠ticas.**
Con base en la forma presentada en la anterior figura y la funci√≥n real utilizada por el modelo Blackbox S, se propuso el siguiente modelo de regresi√≥n no lineal basado en una variante de la funci√≥n $\frac{sin(10x)}{10x}$:

<br>

$$x_2 = \frac{Asin(Bx_1 + C)}{x_1 + 0.1} + D$$

<br>

donde A, B, C y D son par√°metros a ajustar. Se aplicaron los m√©todos de Gauss-Newton y Levenberg-Marquardt para ajustar estos par√°metros utilizando los puntos obtenidos de la frontera inferior puesto que era la m√°s parecida a la forma de la funci√≥n original.

<br>

**3.3. An√°lisis de Resultados**
Ambos m√©todos convergieron a soluciones similares, obteniendo los siguientes par√°metros:
- Levenberg-Marquardt: A = 0.13543566, B = 8.95118215, C = 0.63047492, D = -0.05461683
- Gauss-Newton: A = 0.13543742, B = 8.95141279, C = 0.63044158, D = -0.05461769

Las funciones obtenidas son las siguientes:
- Levenberg-Marquardt:

<br>

$$x_2 = \frac{0.13543566 \cdot sin(8.95118215 \cdot x_1 + 0.63047492)}{x_1 + 0.1} - 0.05461683$$

<br>

- Gauss-Newton:

<br>

$$x_2 = \frac{0.13543742 \cdot sin(8.95141279 \cdot x_1 + 0.63044158)}{x_1 + 0.1} - 0.05461769$$

<br>

A continuaci√≥n, se presenta la comparaci√≥n gr√°fica entre las funciones obtenidas por ambos m√©todos y la funci√≥n real en la Figura 3.

<br>

![Gr√°fica de comparaci√≥n de modelos ajustados y la funci√≥n original](image-2.png)

*Figura 3 Comparaci√≥n del ajuste de la frontera inferior utilizando Gauss-Newton y Levenberg-Marquardt*

<br>

Para comparar los m√©todos utilizados, se utiliz√≥ el error cuadr√°tico medio (MSE). Se emple√≥ esta m√©trica porque el objetivo principal de los m√©todos empleados es reducir el error cuadr√°tico entre los puntos trazados por la funci√≥n real y los valores predichos por el modelo ajustado. Los resultados obtenidos son:

***MSE Levenberg-Marquardt:** 0.0094185092*
***MSE Gauss-Newton:** 0.0094183050*

Con base al error presentado, se concluye que ambos m√©todos generan resultados muy similares en cuanto a su presici√≥n y solo se presentan diferencias en los valores de los par√°metros obtenidos.

<br>

**3.4. An√°lisis de Complejidad Computacional Experimental**

<br>

Se midi√≥ el tiempo de ejecuci√≥n de ambos m√©todos para evaluar su eficiencia computacional. Los resultados obtenidos fueron:
- Tiempo de ejecuci√≥n Levenberg-Marquardt: 0.004609 segundos
- Tiempo de ejecuci√≥n Gauss-Newton: 0.007342 segundos

Estos resultados indican que el m√©todo de Levenberg-Marquardt es m√°s eficiente en t√©rminos de tiempo de ejecuci√≥n en comparaci√≥n con el m√©todo de Gauss-Newton.

---

## 4. Conclusiones y Trabajo Futuro

<br>

**4.1. Resumen de los Hallazgos m√°s Importantes.**

<br>

Este proyecto permiti√≥ identificar la relaci√≥n funcional que establece el modelo Blackbox S entre las variables $x_1$ y $x_2$, a pesar de no conocer una expresi√≥n anal√≠tica interna expl√≠cita del modelo. Mediante un muestreo sistem√°tico y el uso del m√©todo de bisecci√≥n, se determin√≥ con alta precisi√≥n la frontera en la cual el modelo cambia su salida entre 0 y 1, obteniendo dos curvas continuas y suaves que representan los l√≠mites superior e inferior del conjunto donde el modelo predice 1. Una vez obtenidos los puntos experimentales de dichas fronteras, se propuso un modelo funcional basado en una variante de la funci√≥n $sin(x)/x$ o tambi√©n llamada seno cardinal dependiente √∫nicamente de $x_1$ Con el ajuste de par√°metros utilizando los m√©todos Gauss-Newton y Levenberg‚ÄìMarquardt se obtuvo una funci√≥n anal√≠tica aproximada que describe dicha frontera con gran precisi√≥n. Ambos m√©todos convergieron pr√°cticamente al mismo conjunto de par√°metros, lo que valida la estabilidad del modelo matem√°tico elegido y confirma la consistencia de los procedimientos aplicados.

<br>

**4.2. Dificultades Encontradas y Soluciones (CDT y ARV).**

<br>

Durante el desarrollo del proyecto surgieron varias dificultades relevantes:
* Se nos dificult√≥ la identificaci√≥n precisa del punto de cambio entre 0 y 1, inicialmente el muestreo directo no era lo suficientemente preciso para localizar el punto exacto donde el modelo cambia su salida, pero se implemento un procedimiento de bisecci√≥n para definir los l√≠mites hasta alcanzar una tolerancia adecuada
* Tambi√©n fue complicado la elecci√≥n de un modelo anal√≠tico adecuado ya que no se conoc√≠a la forma de la funci√≥n que deb√≠a aproximarse, pero tras analizar la estructura oscilatoria decreciente de los datos, llev√≥ a proponer un modelo basado en una funci√≥n sinc, esta elecci√≥n permiti√≥ que ambos m√©todos de ajuste convergieran correctamente teniendo un ajuste estable y coherente.
* Otra dificultad fue encontrar una estabilidad num√©rica en los ajustes, esto porque el m√©todo Gauss-Newton puede divergir si los valores iniciales no son buenos, para esto se tomaron como valores iniciales par√°metros razonables basados en la forma visual de los datos, esto evit√≥ inestabilidad num√©rica y mejor√≥ la convergencia.

<br>

**4.3. Limitaciones y Restricciones del Enfoque.**

<br>

El enfoque implementado presenta varias limitaciones como, por ejemplo:

* La dependencia total del comportamiento del modelo neuronal ya que la relaci√≥n encontrada no proviene de una deducci√≥n te√≥rica, sino de observar c√≥mo responde el modelo. Si el modelo tuviera ruido, o comportamientos err√°ticos, la aproximaci√≥n ser√≠a menos confiable.
* La funci√≥n ajustada no es la √∫nica posible, existen infinitas funciones que pueden aproximar los puntos obtenidos, la que se escogi√≥ es la adecuada, pero no necesariamente la √∫nica o la √≥ptima en t√©rminos matem√°ticos.
* Tener un dominio restringido ya que el an√°lisis se realiz√≥ dentro de un rango limitado de $x_1$ y $x_2$, fuera de ese rango, no se garantiza la validez del modelo.
* Uso de m√©todos sensibles a los valores iniciales propuestos, porque tanto Gauss-Newton como Levenberg‚ÄìMarquardt requieren buenas condiciones iniciales para converger adecuadamente.
  
<br>

**4.4. Posibles Mejoras y Trabajos Futuros.**

<br>

Existen varias opciones en las que el proyecto se puede ampliar y profundizar el analisis:
* La principal es extender el dominio de an√°lisis ya que, se debe realizar un muestreo m√°s amplio para verificar el comportamiento global del modelo
* Probar utilizando m√©todos de regresi√≥n m√°s avanzados como: regresi√≥n polinomial adaptativa, redes neuronales inversas, modelos simb√≥licos (Symbolic Regression), etc, estos podr√≠an describir una funci√≥n m√°s precisa.
* Evaluar otras funciones base para el ajuste como: fracciones racionales, funciones B-spline, polinomios de Chebyshev, etc, para comparar su desempe√±o frente al modelo tipo $sin(x)/x.$
* Analizar la sensibilidad del modelo, observar c√≥mo peque√±as variaciones en los datos pueden afectar la frontera y el ajuste, esto ayudar√≠a a medir la estabilidad del m√©todo.
